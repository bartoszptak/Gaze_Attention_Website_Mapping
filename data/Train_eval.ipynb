{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "from Model import HourglassNet\n",
    "from Data_utils import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HourglassNet(num_classes=10, \n",
    "                     num_stacks=1, \n",
    "                     num_channels=128, \n",
    "                     inres=(128,128), \n",
    "                     outres=(32,32)).get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=5e-4)\n",
    "loss = tf.keras.metrics.mean_squared_error\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "front_conv_1x1_x1 (Conv2D)      (None, 64, 64, 64)   9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 64, 64)   256         front_conv_1x1_x1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x1_conv_1x1_x1 ( (None, 64, 64, 32)   2080        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 32)   128         front_residual_x1_conv_1x1_x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x1_conv_3x3_x2 ( (None, 64, 64, 32)   9248        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 32)   128         front_residual_x1_conv_3x3_x2[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x1_conv_1x1_x3 ( (None, 64, 64, 64)   2112        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         front_residual_x1_conv_1x1_x3[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x1_residual (Add (None, 64, 64, 64)   0           batch_normalization[0][0]        \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 32, 32, 64)   0           front_residual_x1_residual[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x2_conv_1x1_x1 ( (None, 32, 32, 32)   2080        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         front_residual_x2_conv_1x1_x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x2_conv_3x3_x2 ( (None, 32, 32, 32)   9248        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         front_residual_x2_conv_3x3_x2[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x2_conv_1x1_x3 ( (None, 32, 32, 64)   2112        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         front_residual_x2_conv_1x1_x3[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x2_residual (Add (None, 32, 32, 64)   0           max_pooling2d[0][0]              \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x3_conv_1x1_x1 ( (None, 32, 32, 64)   4160        front_residual_x2_residual[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         front_residual_x3_conv_1x1_x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x3_conv_3x3_x2 ( (None, 32, 32, 64)   36928       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         front_residual_x3_conv_3x3_x2[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x3_conv_1x1_x3 ( (None, 32, 32, 128)  8320        batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x3skip (Conv2D)  (None, 32, 32, 128)  8320        front_residual_x2_residual[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         front_residual_x3_conv_1x1_x3[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x3_residual (Add (None, 32, 32, 128)  0           front_residual_x3skip[0][0]      \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l1_conv_1x1_x1 (Conv2D)     (None, 32, 32, 64)   8256        front_residual_x3_residual[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 64)   256         hg0_l1_conv_1x1_x1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l1_conv_3x3_x2 (Conv2D)     (None, 32, 32, 64)   36928       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 64)   256         hg0_l1_conv_3x3_x2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l1_conv_1x1_x3 (Conv2D)     (None, 32, 32, 128)  8320        batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 128)  512         hg0_l1_conv_1x1_x3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l1_residual (Add)           (None, 32, 32, 128)  0           front_residual_x3_residual[0][0] \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 128)  0           hg0_l1_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l2_conv_1x1_x1 (Conv2D)     (None, 16, 16, 64)   8256        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 64)   256         hg0_l2_conv_1x1_x1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l2_conv_3x3_x2 (Conv2D)     (None, 16, 16, 64)   36928       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         hg0_l2_conv_3x3_x2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l2_conv_1x1_x3 (Conv2D)     (None, 16, 16, 128)  8320        batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 128)  512         hg0_l2_conv_1x1_x3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l2_residual (Add)           (None, 16, 16, 128)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 128)    0           hg0_l2_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l4_conv_1x1_x1 (Conv2D)     (None, 8, 8, 64)     8256        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         hg0_l4_conv_1x1_x1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l4_conv_3x3_x2 (Conv2D)     (None, 8, 8, 64)     36928       batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         hg0_l4_conv_3x3_x2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l4_conv_1x1_x3 (Conv2D)     (None, 8, 8, 128)    8320        batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 128)    512         hg0_l4_conv_1x1_x3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l4_residual (Add)           (None, 8, 8, 128)    0           max_pooling2d_2[0][0]            \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 128)    0           hg0_l4_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l8_conv_1x1_x1 (Conv2D)     (None, 4, 4, 64)     8256        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 4, 4, 64)     256         hg0_l8_conv_1x1_x1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l8_conv_3x3_x2 (Conv2D)     (None, 4, 4, 64)     36928       batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 4, 4, 64)     256         hg0_l8_conv_3x3_x2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l8_conv_1x1_x3 (Conv2D)     (None, 4, 4, 128)    8320        batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 4, 4, 128)    512         hg0_l8_conv_1x1_x3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l8_residual (Add)           (None, 4, 4, 128)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x1_conv_1x1_x1 (Conv2D)   (None, 4, 4, 64)     8256        hg0_l8_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 4, 4, 64)     256         0_lf8_x1_conv_1x1_x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x1_conv_3x3_x2 (Conv2D)   (None, 4, 4, 64)     36928       batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 4, 4, 64)     256         0_lf8_x1_conv_3x3_x2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x1_conv_1x1_x3 (Conv2D)   (None, 4, 4, 128)    8320        batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 4, 4, 128)    512         0_lf8_x1_conv_1x1_x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x1_residual (Add)         (None, 4, 4, 128)    0           hg0_l8_residual[0][0]            \n",
      "                                                                 batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x2_conv_1x1_x1 (Conv2D)   (None, 4, 4, 64)     8256        0_lf8_x1_residual[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 4, 4, 64)     256         0_lf8_x2_conv_1x1_x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x2_conv_3x3_x2 (Conv2D)   (None, 4, 4, 64)     36928       batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 4, 4, 64)     256         0_lf8_x2_conv_3x3_x2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x2_conv_1x1_x3 (Conv2D)   (None, 4, 4, 128)    8320        batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 4, 4, 128)    512         0_lf8_x2_conv_1x1_x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x2_residual (Add)         (None, 4, 4, 128)    0           0_lf8_x1_residual[0][0]          \n",
      "                                                                 batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x3_conv_1x1_x1 (Conv2D)   (None, 4, 4, 64)     8256        0_lf8_x2_residual[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_conv_1x1_x1 (Conv2D)      (None, 4, 4, 64)     8256        hg0_l8_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4, 4, 64)     256         0_lf8_x3_conv_1x1_x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 4, 4, 64)     256         0_lf8_conv_1x1_x1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_conv_1x1_x1 (Co (None, 8, 8, 64)     8256        hg0_l4_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x3_conv_3x3_x2 (Conv2D)   (None, 4, 4, 64)     36928       batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_conv_3x3_x2 (Conv2D)      (None, 4, 4, 64)     36928       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 64)     256         hg0_rf4_connect_conv_1x1_x1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 4, 4, 64)     256         0_lf8_x3_conv_3x3_x2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 4, 4, 64)     256         0_lf8_conv_3x3_x2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_conv_3x3_x2 (Co (None, 8, 8, 64)     36928       batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x3_conv_1x1_x3 (Conv2D)   (None, 4, 4, 128)    8320        batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_conv_1x1_x3 (Conv2D)      (None, 4, 4, 128)    8320        batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 64)     256         hg0_rf4_connect_conv_3x3_x2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 4, 4, 128)    512         0_lf8_x3_conv_1x1_x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 4, 4, 128)    512         0_lf8_conv_1x1_x3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_conv_1x1_x3 (Co (None, 8, 8, 128)    8320        batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x3_residual (Add)         (None, 4, 4, 128)    0           0_lf8_x2_residual[0][0]          \n",
      "                                                                 batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_residual (Add)            (None, 4, 4, 128)    0           hg0_l8_residual[0][0]            \n",
      "                                                                 batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 128)    512         hg0_rf4_connect_conv_1x1_x3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 4, 4, 128)    0           0_lf8_x3_residual[0][0]          \n",
      "                                                                 0_lf8_residual[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_residual (Add)  (None, 8, 8, 128)    0           hg0_l4_residual[0][0]            \n",
      "                                                                 batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 8, 128)    0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 8, 8, 128)    0           hg0_rf4_connect_residual[0][0]   \n",
      "                                                                 up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_conv_conv_1x1_x (None, 8, 8, 64)     8256        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_conv_1x1_x1 (Co (None, 16, 16, 64)   8256        hg0_l2_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 64)     256         hg0_rf4_connect_conv_conv_1x1_x1[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 16, 16, 64)   256         hg0_rf2_connect_conv_1x1_x1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_conv_conv_3x3_x (None, 8, 8, 64)     36928       batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_conv_3x3_x2 (Co (None, 16, 16, 64)   36928       batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 64)     256         hg0_rf4_connect_conv_conv_3x3_x2[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 16, 16, 64)   256         hg0_rf2_connect_conv_3x3_x2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_conv_conv_1x1_x (None, 8, 8, 128)    8320        batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_conv_1x1_x3 (Co (None, 16, 16, 128)  8320        batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 128)    512         hg0_rf4_connect_conv_conv_1x1_x3[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 16, 16, 128)  512         hg0_rf2_connect_conv_1x1_x3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_conv_residual ( (None, 8, 8, 128)    0           add_1[0][0]                      \n",
      "                                                                 batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_residual (Add)  (None, 16, 16, 128)  0           hg0_l2_residual[0][0]            \n",
      "                                                                 batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 128)  0           hg0_rf4_connect_conv_residual[0][\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 128)  0           hg0_rf2_connect_residual[0][0]   \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_conv_conv_1x1_x (None, 16, 16, 64)   8256        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_conv_1x1_x1 (Co (None, 32, 32, 64)   8256        hg0_l1_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 16, 16, 64)   256         hg0_rf2_connect_conv_conv_1x1_x1[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 32, 32, 64)   256         hg0_rf1_connect_conv_1x1_x1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_conv_conv_3x3_x (None, 16, 16, 64)   36928       batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_conv_3x3_x2 (Co (None, 32, 32, 64)   36928       batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 16, 16, 64)   256         hg0_rf2_connect_conv_conv_3x3_x2[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 32, 32, 64)   256         hg0_rf1_connect_conv_3x3_x2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_conv_conv_1x1_x (None, 16, 16, 128)  8320        batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_conv_1x1_x3 (Co (None, 32, 32, 128)  8320        batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 16, 16, 128)  512         hg0_rf2_connect_conv_conv_1x1_x3[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 32, 32, 128)  512         hg0_rf1_connect_conv_1x1_x3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_conv_residual ( (None, 16, 16, 128)  0           add_2[0][0]                      \n",
      "                                                                 batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_residual (Add)  (None, 32, 32, 128)  0           hg0_l1_residual[0][0]            \n",
      "                                                                 batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 128)  0           hg0_rf2_connect_conv_residual[0][\n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 128)  0           hg0_rf1_connect_residual[0][0]   \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_conv_conv_1x1_x (None, 32, 32, 64)   8256        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 32, 32, 64)   256         hg0_rf1_connect_conv_conv_1x1_x1[\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_conv_conv_3x3_x (None, 32, 32, 64)   36928       batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 32, 32, 64)   256         hg0_rf1_connect_conv_conv_3x3_x2[\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_conv_conv_1x1_x (None, 32, 32, 128)  8320        batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 32, 32, 128)  512         hg0_rf1_connect_conv_conv_1x1_x3[\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_conv_residual ( (None, 32, 32, 128)  0           add_3[0][0]                      \n",
      "                                                                 batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_conv_1x1_x1 (Conv2D)          (None, 32, 32, 128)  16512       hg0_rf1_connect_conv_residual[0][\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 32, 32, 128)  512         0_conv_1x1_x1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "0_conv_1x1_parts (Conv2D)       (None, 32, 32, 10)   1290        batch_normalization_52[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 878,090\n",
      "Trainable params: 869,514\n",
      "Non-trainable params: 8,576\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_gen = DataGenerator('dataset/', 'train_landmarks.csv', \n",
    "                          inres=(128,128), outres=(32,32), nparts=10, is_train=True)\n",
    "valid_gen = DataGenerator('dataset/', 'valid_landmarks.csv', \n",
    "                          inres=(128,128), outres=(32,32), nparts=10)\n",
    "test_gen = DataGenerator('dataset/', 'test_landmarks.csv', \n",
    "                         inres=(128,128), outres=(32,32), nparts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_call = [\n",
    "    ModelCheckpoint('model/model.h5', save_best_only=True),\n",
    "    EarlyStopping(patience=20),\n",
    "    TensorBoard(log_dir='train/'),\n",
    "    ReduceLROnPlateau(),\n",
    "    CSVLogger('train/history.csv')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "63/63 [==============================] - 22s 354ms/step - loss: 10.0644 - acc: 0.0973 - val_loss: 2.0920 - val_acc: 0.0725\n",
      "Epoch 2/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.8740 - acc: 0.1057 - val_loss: 3.2296 - val_acc: 0.1974\n",
      "Epoch 3/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.5035 - acc: 0.1103 - val_loss: 3.8304 - val_acc: 0.1706\n",
      "Epoch 4/10000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.3479 - acc: 0.1175 - val_loss: 5.1834 - val_acc: 0.1390\n",
      "Epoch 5/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2724 - acc: 0.1224 - val_loss: 4.5390 - val_acc: 0.0970\n",
      "Epoch 6/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2294 - acc: 0.1263 - val_loss: 5.6183 - val_acc: 0.0978\n",
      "Epoch 7/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.2028 - acc: 0.1318 - val_loss: 1.5238 - val_acc: 0.0672\n",
      "Epoch 8/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1813 - acc: 0.1349 - val_loss: 8.0726 - val_acc: 0.0827\n",
      "Epoch 9/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1671 - acc: 0.1384 - val_loss: 15.2724 - val_acc: 0.0989\n",
      "Epoch 10/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1602 - acc: 0.1418 - val_loss: 104.0057 - val_acc: 0.1293\n",
      "Epoch 11/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.1531 - acc: 0.1418 - val_loss: 0.2207 - val_acc: 0.1231\n",
      "Epoch 12/10000\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.1416 - acc: 0.1482 - val_loss: 0.1582 - val_acc: 0.1193\n",
      "Epoch 13/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.1337 - acc: 0.1525 - val_loss: 0.1393 - val_acc: 0.1269\n",
      "Epoch 14/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.1296 - acc: 0.1556 - val_loss: 0.1324 - val_acc: 0.1654\n",
      "Epoch 15/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.1254 - acc: 0.1575 - val_loss: 0.1257 - val_acc: 0.1555\n",
      "Epoch 16/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.1217 - acc: 0.1584 - val_loss: 0.1216 - val_acc: 0.1643\n",
      "Epoch 17/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.1190 - acc: 0.1626 - val_loss: 0.1185 - val_acc: 0.1931\n",
      "Epoch 18/10000\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.1152 - acc: 0.1651 - val_loss: 0.1141 - val_acc: 0.1583\n",
      "Epoch 19/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.1120 - acc: 0.1693 - val_loss: 0.1115 - val_acc: 0.1501\n",
      "Epoch 20/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.1094 - acc: 0.1721 - val_loss: 0.1089 - val_acc: 0.1554\n",
      "Epoch 21/10000\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.1074 - acc: 0.1747 - val_loss: 0.1065 - val_acc: 0.1611\n",
      "Epoch 22/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1154 - acc: 0.1675 - val_loss: 0.1336 - val_acc: 0.1602\n",
      "Epoch 23/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1094 - acc: 0.1703 - val_loss: 0.1103 - val_acc: 0.1607\n",
      "Epoch 24/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.1053 - acc: 0.1756 - val_loss: 0.1066 - val_acc: 0.1631\n",
      "Epoch 25/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.1030 - acc: 0.1799 - val_loss: 0.1054 - val_acc: 0.1643\n",
      "Epoch 26/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.1012 - acc: 0.1800 - val_loss: 0.1016 - val_acc: 0.1660\n",
      "Epoch 27/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0994 - acc: 0.1842 - val_loss: 0.0999 - val_acc: 0.1655\n",
      "Epoch 28/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0979 - acc: 0.1859 - val_loss: 0.0974 - val_acc: 0.1655\n",
      "Epoch 29/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0967 - acc: 0.1861 - val_loss: 0.0965 - val_acc: 0.1630\n",
      "Epoch 30/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0955 - acc: 0.1894 - val_loss: 0.0949 - val_acc: 0.1672\n",
      "Epoch 31/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0945 - acc: 0.1908 - val_loss: 0.0941 - val_acc: 0.1716\n",
      "Epoch 32/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0934 - acc: 0.1912 - val_loss: 0.0930 - val_acc: 0.1719\n",
      "Epoch 33/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0926 - acc: 0.1934 - val_loss: 0.0928 - val_acc: 0.1773\n",
      "Epoch 34/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0919 - acc: 0.1933 - val_loss: 0.0929 - val_acc: 0.2503\n",
      "Epoch 35/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0915 - acc: 0.1911 - val_loss: 0.0915 - val_acc: 0.2000\n",
      "Epoch 36/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0903 - acc: 0.1945 - val_loss: 0.0896 - val_acc: 0.1771\n",
      "Epoch 37/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0895 - acc: 0.1960 - val_loss: 0.0887 - val_acc: 0.1826\n",
      "Epoch 38/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0886 - acc: 0.1985 - val_loss: 0.0885 - val_acc: 0.1791\n",
      "Epoch 39/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0880 - acc: 0.1985 - val_loss: 0.0877 - val_acc: 0.1800\n",
      "Epoch 40/10000\n",
      "63/63 [==============================] - 4s 71ms/step - loss: 0.0874 - acc: 0.1987 - val_loss: 0.0869 - val_acc: 0.1791\n",
      "Epoch 41/10000\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0868 - acc: 0.2012 - val_loss: 0.0863 - val_acc: 0.1843\n",
      "Epoch 42/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0861 - acc: 0.2022 - val_loss: 0.0855 - val_acc: 0.1918\n",
      "Epoch 43/10000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.0861 - acc: 0.2012 - val_loss: 0.0924 - val_acc: 0.1615\n",
      "Epoch 44/10000\n",
      "63/63 [==============================] - 4s 64ms/step - loss: 0.0880 - acc: 0.1930 - val_loss: 0.0874 - val_acc: 0.1895\n",
      "Epoch 45/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0860 - acc: 0.1988 - val_loss: 0.0856 - val_acc: 0.1945\n",
      "Epoch 46/10000\n",
      "63/63 [==============================] - 5s 74ms/step - loss: 0.0849 - acc: 0.2007 - val_loss: 0.0847 - val_acc: 0.1944\n",
      "Epoch 47/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0841 - acc: 0.2018 - val_loss: 0.0839 - val_acc: 0.1974\n",
      "Epoch 48/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0838 - acc: 0.2012 - val_loss: 0.0838 - val_acc: 0.1808\n",
      "Epoch 49/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0830 - acc: 0.2046 - val_loss: 0.0837 - val_acc: 0.1944\n",
      "Epoch 50/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0823 - acc: 0.2070 - val_loss: 0.0817 - val_acc: 0.2093\n",
      "Epoch 51/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0816 - acc: 0.2072 - val_loss: 0.0813 - val_acc: 0.2040\n",
      "Epoch 52/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0811 - acc: 0.2078 - val_loss: 0.0803 - val_acc: 0.1969\n",
      "Epoch 53/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0805 - acc: 0.2082 - val_loss: 0.0798 - val_acc: 0.2042\n",
      "Epoch 54/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0800 - acc: 0.2104 - val_loss: 0.0793 - val_acc: 0.1994\n",
      "Epoch 55/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0794 - acc: 0.2112 - val_loss: 0.0788 - val_acc: 0.2007\n",
      "Epoch 56/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0788 - acc: 0.2120 - val_loss: 0.0779 - val_acc: 0.1956\n",
      "Epoch 57/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0783 - acc: 0.2149 - val_loss: 0.0773 - val_acc: 0.2017\n",
      "Epoch 58/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0777 - acc: 0.2150 - val_loss: 0.0770 - val_acc: 0.1996\n",
      "Epoch 59/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0771 - acc: 0.2174 - val_loss: 0.0769 - val_acc: 0.1976\n",
      "Epoch 60/10000\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0766 - acc: 0.2185 - val_loss: 0.0763 - val_acc: 0.2051\n",
      "Epoch 61/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0760 - acc: 0.2207 - val_loss: 0.0750 - val_acc: 0.1989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0754 - acc: 0.2163 - val_loss: 0.0745 - val_acc: 0.1981\n",
      "Epoch 63/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0746 - acc: 0.2191 - val_loss: 0.0738 - val_acc: 0.1992\n",
      "Epoch 64/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0741 - acc: 0.2208 - val_loss: 0.0741 - val_acc: 0.2069\n",
      "Epoch 65/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0734 - acc: 0.2215 - val_loss: 0.0737 - val_acc: 0.1985\n",
      "Epoch 66/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0728 - acc: 0.2225 - val_loss: 0.0733 - val_acc: 0.2048\n",
      "Epoch 67/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0730 - acc: 0.2197 - val_loss: 0.0741 - val_acc: 0.2178\n",
      "Epoch 68/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0718 - acc: 0.2200 - val_loss: 0.0722 - val_acc: 0.2143\n",
      "Epoch 69/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0710 - acc: 0.2232 - val_loss: 0.0722 - val_acc: 0.2132\n",
      "Epoch 70/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0703 - acc: 0.2234 - val_loss: 0.0708 - val_acc: 0.2092\n",
      "Epoch 71/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0697 - acc: 0.2214 - val_loss: 0.0705 - val_acc: 0.2062\n",
      "Epoch 72/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0690 - acc: 0.2250 - val_loss: 0.0706 - val_acc: 0.2039\n",
      "Epoch 73/10000\n",
      "63/63 [==============================] - 4s 71ms/step - loss: 0.0688 - acc: 0.2243 - val_loss: 0.0705 - val_acc: 0.2076\n",
      "Epoch 74/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0679 - acc: 0.2243 - val_loss: 0.0694 - val_acc: 0.2076\n",
      "Epoch 75/10000\n",
      "63/63 [==============================] - 4s 71ms/step - loss: 0.0671 - acc: 0.2247 - val_loss: 0.0690 - val_acc: 0.2131\n",
      "Epoch 76/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0664 - acc: 0.2228 - val_loss: 0.0682 - val_acc: 0.2103\n",
      "Epoch 77/10000\n",
      "63/63 [==============================] - 4s 71ms/step - loss: 0.0659 - acc: 0.2228 - val_loss: 0.0677 - val_acc: 0.2043\n",
      "Epoch 78/10000\n",
      "63/63 [==============================] - 4s 71ms/step - loss: 0.0651 - acc: 0.2256 - val_loss: 0.0675 - val_acc: 0.2077\n",
      "Epoch 79/10000\n",
      "63/63 [==============================] - 4s 71ms/step - loss: 0.0644 - acc: 0.2239 - val_loss: 0.0659 - val_acc: 0.2025\n",
      "Epoch 80/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0638 - acc: 0.2251 - val_loss: 0.0669 - val_acc: 0.1926\n",
      "Epoch 81/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0633 - acc: 0.2240 - val_loss: 0.0664 - val_acc: 0.1892\n",
      "Epoch 82/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0626 - acc: 0.2239 - val_loss: 0.0646 - val_acc: 0.2065\n",
      "Epoch 83/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0626 - acc: 0.2264 - val_loss: 0.0691 - val_acc: 0.2552\n",
      "Epoch 84/10000\n",
      "63/63 [==============================] - 5s 71ms/step - loss: 0.0625 - acc: 0.2260 - val_loss: 0.0646 - val_acc: 0.2063\n",
      "Epoch 85/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0614 - acc: 0.2297 - val_loss: 0.0634 - val_acc: 0.1960\n",
      "Epoch 86/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0606 - acc: 0.2272 - val_loss: 0.0635 - val_acc: 0.1944\n",
      "Epoch 87/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0599 - acc: 0.2281 - val_loss: 0.0627 - val_acc: 0.2061\n",
      "Epoch 88/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0594 - acc: 0.2300 - val_loss: 0.0614 - val_acc: 0.2081\n",
      "Epoch 89/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0589 - acc: 0.2309 - val_loss: 0.0623 - val_acc: 0.1821\n",
      "Epoch 90/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0583 - acc: 0.2272 - val_loss: 0.0627 - val_acc: 0.1882\n",
      "Epoch 91/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0577 - acc: 0.2304 - val_loss: 0.0614 - val_acc: 0.1970\n",
      "Epoch 92/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0573 - acc: 0.2288 - val_loss: 0.0610 - val_acc: 0.1968\n",
      "Epoch 93/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0567 - acc: 0.2300 - val_loss: 0.0610 - val_acc: 0.1786\n",
      "Epoch 94/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0564 - acc: 0.2285 - val_loss: 0.0604 - val_acc: 0.1817\n",
      "Epoch 95/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0559 - acc: 0.2270 - val_loss: 0.0606 - val_acc: 0.2001\n",
      "Epoch 96/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0553 - acc: 0.2274 - val_loss: 0.0610 - val_acc: 0.1862\n",
      "Epoch 97/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0549 - acc: 0.2269 - val_loss: 0.0616 - val_acc: 0.1957\n",
      "Epoch 98/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0545 - acc: 0.2272 - val_loss: 0.0613 - val_acc: 0.1977\n",
      "Epoch 99/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0540 - acc: 0.2269 - val_loss: 0.0606 - val_acc: 0.1778\n",
      "Epoch 100/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0538 - acc: 0.2252 - val_loss: 0.0610 - val_acc: 0.1705\n",
      "Epoch 101/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0540 - acc: 0.2249 - val_loss: 0.0585 - val_acc: 0.1925\n",
      "Epoch 102/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0528 - acc: 0.2259 - val_loss: 0.0599 - val_acc: 0.1911\n",
      "Epoch 103/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0524 - acc: 0.2232 - val_loss: 0.0594 - val_acc: 0.1901\n",
      "Epoch 104/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0519 - acc: 0.2260 - val_loss: 0.0586 - val_acc: 0.2021\n",
      "Epoch 105/10000\n",
      "63/63 [==============================] - 4s 71ms/step - loss: 0.0518 - acc: 0.2243 - val_loss: 0.0573 - val_acc: 0.1987\n",
      "Epoch 106/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0513 - acc: 0.2243 - val_loss: 0.0584 - val_acc: 0.1985\n",
      "Epoch 107/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0508 - acc: 0.2247 - val_loss: 0.0584 - val_acc: 0.1967\n",
      "Epoch 108/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0503 - acc: 0.2220 - val_loss: 0.0573 - val_acc: 0.2029\n",
      "Epoch 109/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0498 - acc: 0.2225 - val_loss: 0.0581 - val_acc: 0.2181\n",
      "Epoch 110/10000\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 0.0496 - acc: 0.2215 - val_loss: 0.0571 - val_acc: 0.2127\n",
      "Epoch 111/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0495 - acc: 0.2231 - val_loss: 0.0590 - val_acc: 0.1723\n",
      "Epoch 112/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0491 - acc: 0.2251 - val_loss: 0.0595 - val_acc: 0.2638\n",
      "Epoch 113/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0487 - acc: 0.2252 - val_loss: 0.0594 - val_acc: 0.1843\n",
      "Epoch 114/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0486 - acc: 0.2214 - val_loss: 0.0586 - val_acc: 0.1964\n",
      "Epoch 115/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0477 - acc: 0.2243 - val_loss: 0.0603 - val_acc: 0.2118\n",
      "Epoch 116/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0477 - acc: 0.2246 - val_loss: 0.0602 - val_acc: 0.2001\n",
      "Epoch 117/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0471 - acc: 0.2291 - val_loss: 0.0606 - val_acc: 0.1829\n",
      "Epoch 118/10000\n",
      "63/63 [==============================] - 5s 72ms/step - loss: 0.0470 - acc: 0.2254 - val_loss: 0.0566 - val_acc: 0.2511\n",
      "Epoch 119/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0463 - acc: 0.2235 - val_loss: 0.0585 - val_acc: 0.2009\n",
      "Epoch 120/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0845 - acc: 0.2065 - val_loss: 3.9806 - val_acc: 0.1261\n",
      "Epoch 121/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.2105 - acc: 0.1192 - val_loss: 15.0493 - val_acc: 0.0336\n",
      "Epoch 122/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.1015 - acc: 0.1464 - val_loss: 1.4744 - val_acc: 0.0563\n",
      "Epoch 123/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0926 - acc: 0.1655 - val_loss: 0.1367 - val_acc: 0.0739\n",
      "Epoch 124/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0898 - acc: 0.1757 - val_loss: 0.0995 - val_acc: 0.1088\n",
      "Epoch 125/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0876 - acc: 0.1814 - val_loss: 0.0903 - val_acc: 0.1291\n",
      "Epoch 126/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0856 - acc: 0.1826 - val_loss: 0.0864 - val_acc: 0.1359\n",
      "Epoch 127/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0832 - acc: 0.1871 - val_loss: 0.0853 - val_acc: 0.1443\n",
      "Epoch 128/10000\n",
      "63/63 [==============================] - 4s 69ms/step - loss: 0.0807 - acc: 0.1930 - val_loss: 0.0841 - val_acc: 0.1469\n",
      "Epoch 129/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0790 - acc: 0.1921 - val_loss: 0.0811 - val_acc: 0.1572\n",
      "Epoch 130/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0787 - acc: 0.1930 - val_loss: 0.0800 - val_acc: 0.1659\n",
      "Epoch 131/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0785 - acc: 0.1938 - val_loss: 0.0792 - val_acc: 0.1712\n",
      "Epoch 132/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0783 - acc: 0.1930 - val_loss: 0.0782 - val_acc: 0.1759\n",
      "Epoch 133/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0781 - acc: 0.1938 - val_loss: 0.0780 - val_acc: 0.1805\n",
      "Epoch 134/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0779 - acc: 0.1934 - val_loss: 0.0771 - val_acc: 0.1816\n",
      "Epoch 135/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0776 - acc: 0.1942 - val_loss: 0.0767 - val_acc: 0.1776\n",
      "Epoch 136/10000\n",
      "63/63 [==============================] - 4s 63ms/step - loss: 0.0774 - acc: 0.1938 - val_loss: 0.0765 - val_acc: 0.1852\n",
      "Epoch 137/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0771 - acc: 0.1953 - val_loss: 0.0759 - val_acc: 0.1803\n",
      "Epoch 138/10000\n",
      "63/63 [==============================] - 4s 62ms/step - loss: 0.0769 - acc: 0.1946 - val_loss: 0.0758 - val_acc: 0.1829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f03e479cc18>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model.fit_generator(train_gen.generator(batch_size, 1), len(train_gen)//batch_size,\n",
    "                    validation_data=valid_gen.generator(batch_size, 1), \n",
    "                    validation_steps=len(valid_gen)//batch_size,\n",
    "                    epochs=10000, \n",
    "                    callbacks=my_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.002380064756835054, 0.1788330078125]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_gen.generator(1,1), steps=len(test_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
