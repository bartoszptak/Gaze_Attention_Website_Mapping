{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "from Model import HourglassNet\n",
    "from Data_utils import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HourglassNet(num_classes=7, \n",
    "                     num_stacks=1, \n",
    "                     num_channels=128, \n",
    "                     inres=(128,128), \n",
    "                     outres=(32,32)).get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=5e-4)\n",
    "loss = tf.keras.metrics.mean_squared_error\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 128, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "front_conv_1x1_x1 (Conv2D)      (None, 64, 64, 64)   3200        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 64, 64)   256         front_conv_1x1_x1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x1_conv_1x1_x1 ( (None, 64, 64, 32)   2080        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 32)   128         front_residual_x1_conv_1x1_x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x1_conv_3x3_x2 ( (None, 64, 64, 32)   9248        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 32)   128         front_residual_x1_conv_3x3_x2[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x1_conv_1x1_x3 ( (None, 64, 64, 64)   2112        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         front_residual_x1_conv_1x1_x3[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x1_residual (Add (None, 64, 64, 64)   0           batch_normalization[0][0]        \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 32, 32, 64)   0           front_residual_x1_residual[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x2_conv_1x1_x1 ( (None, 32, 32, 32)   2080        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         front_residual_x2_conv_1x1_x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x2_conv_3x3_x2 ( (None, 32, 32, 32)   9248        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         front_residual_x2_conv_3x3_x2[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x2_conv_1x1_x3 ( (None, 32, 32, 64)   2112        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         front_residual_x2_conv_1x1_x3[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x2_residual (Add (None, 32, 32, 64)   0           max_pooling2d[0][0]              \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x3_conv_1x1_x1 ( (None, 32, 32, 64)   4160        front_residual_x2_residual[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         front_residual_x3_conv_1x1_x1[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x3_conv_3x3_x2 ( (None, 32, 32, 64)   36928       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         front_residual_x3_conv_3x3_x2[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x3_conv_1x1_x3 ( (None, 32, 32, 128)  8320        batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x3skip (Conv2D)  (None, 32, 32, 128)  8320        front_residual_x2_residual[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         front_residual_x3_conv_1x1_x3[0][\n",
      "__________________________________________________________________________________________________\n",
      "front_residual_x3_residual (Add (None, 32, 32, 128)  0           front_residual_x3skip[0][0]      \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l1_conv_1x1_x1 (Conv2D)     (None, 32, 32, 64)   8256        front_residual_x3_residual[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 64)   256         hg0_l1_conv_1x1_x1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l1_conv_3x3_x2 (Conv2D)     (None, 32, 32, 64)   36928       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 64)   256         hg0_l1_conv_3x3_x2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l1_conv_1x1_x3 (Conv2D)     (None, 32, 32, 128)  8320        batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 128)  512         hg0_l1_conv_1x1_x3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l1_residual (Add)           (None, 32, 32, 128)  0           front_residual_x3_residual[0][0] \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 128)  0           hg0_l1_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l2_conv_1x1_x1 (Conv2D)     (None, 16, 16, 64)   8256        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 64)   256         hg0_l2_conv_1x1_x1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l2_conv_3x3_x2 (Conv2D)     (None, 16, 16, 64)   36928       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         hg0_l2_conv_3x3_x2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l2_conv_1x1_x3 (Conv2D)     (None, 16, 16, 128)  8320        batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 128)  512         hg0_l2_conv_1x1_x3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l2_residual (Add)           (None, 16, 16, 128)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 128)    0           hg0_l2_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l4_conv_1x1_x1 (Conv2D)     (None, 8, 8, 64)     8256        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         hg0_l4_conv_1x1_x1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l4_conv_3x3_x2 (Conv2D)     (None, 8, 8, 64)     36928       batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         hg0_l4_conv_3x3_x2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l4_conv_1x1_x3 (Conv2D)     (None, 8, 8, 128)    8320        batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 128)    512         hg0_l4_conv_1x1_x3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l4_residual (Add)           (None, 8, 8, 128)    0           max_pooling2d_2[0][0]            \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 128)    0           hg0_l4_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l8_conv_1x1_x1 (Conv2D)     (None, 4, 4, 64)     8256        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 4, 4, 64)     256         hg0_l8_conv_1x1_x1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l8_conv_3x3_x2 (Conv2D)     (None, 4, 4, 64)     36928       batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 4, 4, 64)     256         hg0_l8_conv_3x3_x2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l8_conv_1x1_x3 (Conv2D)     (None, 4, 4, 128)    8320        batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 4, 4, 128)    512         hg0_l8_conv_1x1_x3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hg0_l8_residual (Add)           (None, 4, 4, 128)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x1_conv_1x1_x1 (Conv2D)   (None, 4, 4, 64)     8256        hg0_l8_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 4, 4, 64)     256         0_lf8_x1_conv_1x1_x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x1_conv_3x3_x2 (Conv2D)   (None, 4, 4, 64)     36928       batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 4, 4, 64)     256         0_lf8_x1_conv_3x3_x2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x1_conv_1x1_x3 (Conv2D)   (None, 4, 4, 128)    8320        batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 4, 4, 128)    512         0_lf8_x1_conv_1x1_x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x1_residual (Add)         (None, 4, 4, 128)    0           hg0_l8_residual[0][0]            \n",
      "                                                                 batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x2_conv_1x1_x1 (Conv2D)   (None, 4, 4, 64)     8256        0_lf8_x1_residual[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 4, 4, 64)     256         0_lf8_x2_conv_1x1_x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x2_conv_3x3_x2 (Conv2D)   (None, 4, 4, 64)     36928       batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 4, 4, 64)     256         0_lf8_x2_conv_3x3_x2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x2_conv_1x1_x3 (Conv2D)   (None, 4, 4, 128)    8320        batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 4, 4, 128)    512         0_lf8_x2_conv_1x1_x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x2_residual (Add)         (None, 4, 4, 128)    0           0_lf8_x1_residual[0][0]          \n",
      "                                                                 batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x3_conv_1x1_x1 (Conv2D)   (None, 4, 4, 64)     8256        0_lf8_x2_residual[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_conv_1x1_x1 (Conv2D)      (None, 4, 4, 64)     8256        hg0_l8_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4, 4, 64)     256         0_lf8_x3_conv_1x1_x1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 4, 4, 64)     256         0_lf8_conv_1x1_x1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_conv_1x1_x1 (Co (None, 8, 8, 64)     8256        hg0_l4_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x3_conv_3x3_x2 (Conv2D)   (None, 4, 4, 64)     36928       batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_conv_3x3_x2 (Conv2D)      (None, 4, 4, 64)     36928       batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 64)     256         hg0_rf4_connect_conv_1x1_x1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 4, 4, 64)     256         0_lf8_x3_conv_3x3_x2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 4, 4, 64)     256         0_lf8_conv_3x3_x2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_conv_3x3_x2 (Co (None, 8, 8, 64)     36928       batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x3_conv_1x1_x3 (Conv2D)   (None, 4, 4, 128)    8320        batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_conv_1x1_x3 (Conv2D)      (None, 4, 4, 128)    8320        batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 64)     256         hg0_rf4_connect_conv_3x3_x2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 4, 4, 128)    512         0_lf8_x3_conv_1x1_x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 4, 4, 128)    512         0_lf8_conv_1x1_x3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_conv_1x1_x3 (Co (None, 8, 8, 128)    8320        batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_x3_residual (Add)         (None, 4, 4, 128)    0           0_lf8_x2_residual[0][0]          \n",
      "                                                                 batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_lf8_residual (Add)            (None, 4, 4, 128)    0           hg0_l8_residual[0][0]            \n",
      "                                                                 batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 128)    512         hg0_rf4_connect_conv_1x1_x3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 4, 4, 128)    0           0_lf8_x3_residual[0][0]          \n",
      "                                                                 0_lf8_residual[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_residual (Add)  (None, 8, 8, 128)    0           hg0_l4_residual[0][0]            \n",
      "                                                                 batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 8, 128)    0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 8, 8, 128)    0           hg0_rf4_connect_residual[0][0]   \n",
      "                                                                 up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_conv_conv_1x1_x (None, 8, 8, 64)     8256        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_conv_1x1_x1 (Co (None, 16, 16, 64)   8256        hg0_l2_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 64)     256         hg0_rf4_connect_conv_conv_1x1_x1[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 16, 16, 64)   256         hg0_rf2_connect_conv_1x1_x1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_conv_conv_3x3_x (None, 8, 8, 64)     36928       batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_conv_3x3_x2 (Co (None, 16, 16, 64)   36928       batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 64)     256         hg0_rf4_connect_conv_conv_3x3_x2[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 16, 16, 64)   256         hg0_rf2_connect_conv_3x3_x2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_conv_conv_1x1_x (None, 8, 8, 128)    8320        batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_conv_1x1_x3 (Co (None, 16, 16, 128)  8320        batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 128)    512         hg0_rf4_connect_conv_conv_1x1_x3[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 16, 16, 128)  512         hg0_rf2_connect_conv_1x1_x3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf4_connect_conv_residual ( (None, 8, 8, 128)    0           add_1[0][0]                      \n",
      "                                                                 batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_residual (Add)  (None, 16, 16, 128)  0           hg0_l2_residual[0][0]            \n",
      "                                                                 batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 128)  0           hg0_rf4_connect_conv_residual[0][\n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 128)  0           hg0_rf2_connect_residual[0][0]   \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_conv_conv_1x1_x (None, 16, 16, 64)   8256        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_conv_1x1_x1 (Co (None, 32, 32, 64)   8256        hg0_l1_residual[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 16, 16, 64)   256         hg0_rf2_connect_conv_conv_1x1_x1[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 32, 32, 64)   256         hg0_rf1_connect_conv_1x1_x1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_conv_conv_3x3_x (None, 16, 16, 64)   36928       batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_conv_3x3_x2 (Co (None, 32, 32, 64)   36928       batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 16, 16, 64)   256         hg0_rf2_connect_conv_conv_3x3_x2[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 32, 32, 64)   256         hg0_rf1_connect_conv_3x3_x2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_conv_conv_1x1_x (None, 16, 16, 128)  8320        batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_conv_1x1_x3 (Co (None, 32, 32, 128)  8320        batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 16, 16, 128)  512         hg0_rf2_connect_conv_conv_1x1_x3[\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 32, 32, 128)  512         hg0_rf1_connect_conv_1x1_x3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf2_connect_conv_residual ( (None, 16, 16, 128)  0           add_2[0][0]                      \n",
      "                                                                 batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_residual (Add)  (None, 32, 32, 128)  0           hg0_l1_residual[0][0]            \n",
      "                                                                 batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 128)  0           hg0_rf2_connect_conv_residual[0][\n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 128)  0           hg0_rf1_connect_residual[0][0]   \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_conv_conv_1x1_x (None, 32, 32, 64)   8256        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 32, 32, 64)   256         hg0_rf1_connect_conv_conv_1x1_x1[\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_conv_conv_3x3_x (None, 32, 32, 64)   36928       batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 32, 32, 64)   256         hg0_rf1_connect_conv_conv_3x3_x2[\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_conv_conv_1x1_x (None, 32, 32, 128)  8320        batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 32, 32, 128)  512         hg0_rf1_connect_conv_conv_1x1_x3[\n",
      "__________________________________________________________________________________________________\n",
      "hg0_rf1_connect_conv_residual ( (None, 32, 32, 128)  0           add_3[0][0]                      \n",
      "                                                                 batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "0_conv_1x1_x1 (Conv2D)          (None, 32, 32, 128)  16512       hg0_rf1_connect_conv_residual[0][\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 32, 32, 128)  512         0_conv_1x1_x1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "0_conv_1x1_parts (Conv2D)       (None, 32, 32, 7)    903         batch_normalization_52[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 871,431\n",
      "Trainable params: 862,855\n",
      "Non-trainable params: 8,576\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_gen = DataGenerator('dataset/', 'train_landmarks.csv', \n",
    "                          inres=(128,128), outres=(32,32), nparts=7, is_train=True)\n",
    "valid_gen = DataGenerator('dataset/', 'valid_landmarks.csv', \n",
    "                          inres=(128,128), outres=(32,32), nparts=7)\n",
    "test_gen = DataGenerator('dataset/', 'test_landmarks.csv', \n",
    "                         inres=(128,128), outres=(32,32), nparts=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_call = [\n",
    "    ModelCheckpoint('model/model.h5', save_best_only=True),\n",
    "    EarlyStopping(patience=40),\n",
    "    TensorBoard(log_dir='train/'),\n",
    "    ReduceLROnPlateau(patience=20),\n",
    "    CSVLogger('train/history.csv')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "63/63 [==============================] - 38s 603ms/step - loss: 11.2466 - acc: 0.1541 - val_loss: 11.0483 - val_acc: 0.1338\n",
      "Epoch 2/10000\n",
      "63/63 [==============================] - 15s 236ms/step - loss: 1.0321 - acc: 0.1652 - val_loss: 38.0499 - val_acc: 0.1047\n",
      "Epoch 3/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.6174 - acc: 0.1773 - val_loss: 39.6428 - val_acc: 0.0931\n",
      "Epoch 4/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.4419 - acc: 0.1939 - val_loss: 14.8531 - val_acc: 0.2084\n",
      "Epoch 5/10000\n",
      "63/63 [==============================] - 15s 242ms/step - loss: 0.3498 - acc: 0.2007 - val_loss: 5.4694 - val_acc: 0.4583\n",
      "Epoch 6/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.2953 - acc: 0.2128 - val_loss: 2.1197 - val_acc: 0.4450\n",
      "Epoch 7/10000\n",
      "63/63 [==============================] - 15s 244ms/step - loss: 0.2587 - acc: 0.2186 - val_loss: 0.8697 - val_acc: 0.3191\n",
      "Epoch 8/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.2325 - acc: 0.2251 - val_loss: 0.6484 - val_acc: 0.2381\n",
      "Epoch 9/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.2114 - acc: 0.2332 - val_loss: 0.6678 - val_acc: 0.2342\n",
      "Epoch 10/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.1988 - acc: 0.2460 - val_loss: 0.6753 - val_acc: 0.2019\n",
      "Epoch 11/10000\n",
      "63/63 [==============================] - 15s 242ms/step - loss: 0.1839 - acc: 0.2467 - val_loss: 0.6282 - val_acc: 0.2115\n",
      "Epoch 12/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1761 - acc: 0.2524 - val_loss: 0.2674 - val_acc: 0.1952\n",
      "Epoch 13/10000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.1677 - acc: 0.2571 - val_loss: 0.1953 - val_acc: 0.2034\n",
      "Epoch 14/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1596 - acc: 0.2562 - val_loss: 0.1684 - val_acc: 0.1968\n",
      "Epoch 15/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1540 - acc: 0.2642 - val_loss: 0.1574 - val_acc: 0.2083\n",
      "Epoch 16/10000\n",
      "63/63 [==============================] - 15s 242ms/step - loss: 0.1479 - acc: 0.2650 - val_loss: 0.1509 - val_acc: 0.2126\n",
      "Epoch 17/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1437 - acc: 0.2674 - val_loss: 0.1429 - val_acc: 0.2236\n",
      "Epoch 18/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.1404 - acc: 0.2761 - val_loss: 0.1431 - val_acc: 0.2092\n",
      "Epoch 19/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1366 - acc: 0.2715 - val_loss: 0.1368 - val_acc: 0.2314\n",
      "Epoch 20/10000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.1332 - acc: 0.2773 - val_loss: 0.1317 - val_acc: 0.2398\n",
      "Epoch 21/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1301 - acc: 0.2665 - val_loss: 0.1298 - val_acc: 0.2550\n",
      "Epoch 22/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1277 - acc: 0.2706 - val_loss: 0.1256 - val_acc: 0.2493\n",
      "Epoch 23/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1252 - acc: 0.2797 - val_loss: 0.1253 - val_acc: 0.2549\n",
      "Epoch 24/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1235 - acc: 0.2852 - val_loss: 0.1236 - val_acc: 0.2502\n",
      "Epoch 25/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1213 - acc: 0.2818 - val_loss: 0.1209 - val_acc: 0.2972\n",
      "Epoch 26/10000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.1196 - acc: 0.2830 - val_loss: 0.1194 - val_acc: 0.2927\n",
      "Epoch 27/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1172 - acc: 0.2853 - val_loss: 0.1160 - val_acc: 0.2524\n",
      "Epoch 28/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.1178 - acc: 0.2857 - val_loss: 0.1355 - val_acc: 0.2234\n",
      "Epoch 29/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1163 - acc: 0.2816 - val_loss: 0.1146 - val_acc: 0.2608\n",
      "Epoch 30/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1136 - acc: 0.2805 - val_loss: 0.1135 - val_acc: 0.2900\n",
      "Epoch 31/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1120 - acc: 0.2855 - val_loss: 0.1131 - val_acc: 0.2619\n",
      "Epoch 32/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1106 - acc: 0.2827 - val_loss: 0.1089 - val_acc: 0.2987\n",
      "Epoch 33/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1095 - acc: 0.2903 - val_loss: 0.1085 - val_acc: 0.2811\n",
      "Epoch 34/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1078 - acc: 0.2858 - val_loss: 0.1065 - val_acc: 0.3018\n",
      "Epoch 35/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1066 - acc: 0.2820 - val_loss: 0.1059 - val_acc: 0.2873\n",
      "Epoch 36/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.1052 - acc: 0.2952 - val_loss: 0.1070 - val_acc: 0.3103\n",
      "Epoch 37/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1043 - acc: 0.2911 - val_loss: 0.1057 - val_acc: 0.2810\n",
      "Epoch 38/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1027 - acc: 0.2913 - val_loss: 0.1022 - val_acc: 0.3201\n",
      "Epoch 39/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1014 - acc: 0.2901 - val_loss: 0.1010 - val_acc: 0.3107\n",
      "Epoch 40/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.1009 - acc: 0.2906 - val_loss: 0.0989 - val_acc: 0.3038\n",
      "Epoch 41/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0993 - acc: 0.2970 - val_loss: 0.0985 - val_acc: 0.3293\n",
      "Epoch 42/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0980 - acc: 0.2908 - val_loss: 0.0982 - val_acc: 0.2825\n",
      "Epoch 43/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0970 - acc: 0.2967 - val_loss: 0.0990 - val_acc: 0.2806\n",
      "Epoch 44/10000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.0961 - acc: 0.2877 - val_loss: 0.0971 - val_acc: 0.3045\n",
      "Epoch 45/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0948 - acc: 0.2961 - val_loss: 0.0957 - val_acc: 0.2889\n",
      "Epoch 46/10000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.0940 - acc: 0.3005 - val_loss: 0.0935 - val_acc: 0.2905\n",
      "Epoch 47/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0928 - acc: 0.2958 - val_loss: 0.0920 - val_acc: 0.3039\n",
      "Epoch 48/10000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.0920 - acc: 0.2924 - val_loss: 0.0910 - val_acc: 0.3044\n",
      "Epoch 49/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0910 - acc: 0.2988 - val_loss: 0.0911 - val_acc: 0.2788\n",
      "Epoch 50/10000\n",
      "63/63 [==============================] - 15s 242ms/step - loss: 0.0905 - acc: 0.3012 - val_loss: 0.0902 - val_acc: 0.2507\n",
      "Epoch 51/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0892 - acc: 0.2924 - val_loss: 0.0922 - val_acc: 0.3325\n",
      "Epoch 52/10000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.0888 - acc: 0.3073 - val_loss: 0.0886 - val_acc: 0.2852\n",
      "Epoch 53/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0876 - acc: 0.2946 - val_loss: 0.0889 - val_acc: 0.2742\n",
      "Epoch 54/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0866 - acc: 0.3103 - val_loss: 0.0882 - val_acc: 0.2721\n",
      "Epoch 55/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0862 - acc: 0.3087 - val_loss: 0.0865 - val_acc: 0.2808\n",
      "Epoch 56/10000\n",
      "63/63 [==============================] - 15s 242ms/step - loss: 0.0856 - acc: 0.2997 - val_loss: 0.0864 - val_acc: 0.3287\n",
      "Epoch 57/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0845 - acc: 0.2988 - val_loss: 0.0843 - val_acc: 0.3225\n",
      "Epoch 58/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0837 - acc: 0.2904 - val_loss: 0.0845 - val_acc: 0.3722\n",
      "Epoch 59/10000\n",
      "63/63 [==============================] - 15s 242ms/step - loss: 0.0827 - acc: 0.3029 - val_loss: 0.0841 - val_acc: 0.3355\n",
      "Epoch 60/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0819 - acc: 0.3090 - val_loss: 0.0843 - val_acc: 0.3416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0814 - acc: 0.3159 - val_loss: 0.0835 - val_acc: 0.3314\n",
      "Epoch 62/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0805 - acc: 0.3073 - val_loss: 0.0825 - val_acc: 0.3348\n",
      "Epoch 63/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0793 - acc: 0.3339 - val_loss: 0.0811 - val_acc: 0.3048\n",
      "Epoch 64/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0787 - acc: 0.3045 - val_loss: 0.0803 - val_acc: 0.3727\n",
      "Epoch 65/10000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.0780 - acc: 0.3058 - val_loss: 0.0783 - val_acc: 0.3254\n",
      "Epoch 66/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0773 - acc: 0.3173 - val_loss: 0.0786 - val_acc: 0.2527\n",
      "Epoch 67/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0767 - acc: 0.2992 - val_loss: 0.0765 - val_acc: 0.3164\n",
      "Epoch 68/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0759 - acc: 0.3092 - val_loss: 0.0786 - val_acc: 0.2967\n",
      "Epoch 69/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0754 - acc: 0.3093 - val_loss: 0.0788 - val_acc: 0.3266\n",
      "Epoch 70/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0751 - acc: 0.3051 - val_loss: 0.0827 - val_acc: 0.2190\n",
      "Epoch 71/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0742 - acc: 0.2946 - val_loss: 0.0774 - val_acc: 0.2396\n",
      "Epoch 72/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0732 - acc: 0.3064 - val_loss: 0.0786 - val_acc: 0.2548\n",
      "Epoch 73/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0723 - acc: 0.3026 - val_loss: 0.0747 - val_acc: 0.2704\n",
      "Epoch 74/10000\n",
      "63/63 [==============================] - 15s 244ms/step - loss: 0.0715 - acc: 0.3008 - val_loss: 0.0742 - val_acc: 0.2750\n",
      "Epoch 75/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0709 - acc: 0.3102 - val_loss: 0.0763 - val_acc: 0.2453\n",
      "Epoch 76/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0703 - acc: 0.3103 - val_loss: 0.0756 - val_acc: 0.2650\n",
      "Epoch 77/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0698 - acc: 0.3102 - val_loss: 0.0739 - val_acc: 0.2454\n",
      "Epoch 78/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0693 - acc: 0.3075 - val_loss: 0.0754 - val_acc: 0.3411\n",
      "Epoch 79/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0687 - acc: 0.3078 - val_loss: 0.0767 - val_acc: 0.2961\n",
      "Epoch 80/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0675 - acc: 0.3026 - val_loss: 0.0695 - val_acc: 0.3735\n",
      "Epoch 81/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0672 - acc: 0.3164 - val_loss: 0.0762 - val_acc: 0.2781\n",
      "Epoch 82/10000\n",
      "63/63 [==============================] - 14s 230ms/step - loss: 0.0665 - acc: 0.3126 - val_loss: 0.0711 - val_acc: 0.3235\n",
      "Epoch 83/10000\n",
      "63/63 [==============================] - 14s 230ms/step - loss: 0.0654 - acc: 0.3161 - val_loss: 0.0721 - val_acc: 0.3449\n",
      "Epoch 84/10000\n",
      "63/63 [==============================] - 14s 230ms/step - loss: 0.0650 - acc: 0.3242 - val_loss: 0.0714 - val_acc: 0.2582\n",
      "Epoch 85/10000\n",
      "63/63 [==============================] - 14s 230ms/step - loss: 0.0642 - acc: 0.3228 - val_loss: 0.0750 - val_acc: 0.2308\n",
      "Epoch 86/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0638 - acc: 0.3178 - val_loss: 0.0749 - val_acc: 0.2439\n",
      "Epoch 87/10000\n",
      "63/63 [==============================] - 14s 230ms/step - loss: 0.0631 - acc: 0.3104 - val_loss: 0.0729 - val_acc: 0.2655\n",
      "Epoch 88/10000\n",
      "63/63 [==============================] - 14s 230ms/step - loss: 0.0624 - acc: 0.3085 - val_loss: 0.0709 - val_acc: 0.2897\n",
      "Epoch 89/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0623 - acc: 0.3111 - val_loss: 0.0733 - val_acc: 0.2444\n",
      "Epoch 90/10000\n",
      "63/63 [==============================] - 15s 242ms/step - loss: 0.0610 - acc: 0.3201 - val_loss: 0.0683 - val_acc: 0.2549\n",
      "Epoch 91/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0607 - acc: 0.3166 - val_loss: 0.0724 - val_acc: 0.2721\n",
      "Epoch 92/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0602 - acc: 0.3218 - val_loss: 0.0709 - val_acc: 0.2312\n",
      "Epoch 93/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0595 - acc: 0.3110 - val_loss: 0.0755 - val_acc: 0.2461\n",
      "Epoch 94/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0591 - acc: 0.3245 - val_loss: 0.0696 - val_acc: 0.2586\n",
      "Epoch 95/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0584 - acc: 0.3182 - val_loss: 0.0770 - val_acc: 0.3436\n",
      "Epoch 96/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0616 - acc: 0.3118 - val_loss: 0.2382 - val_acc: 0.2945\n",
      "Epoch 97/10000\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0648 - acc: 0.3130 - val_loss: 0.0665 - val_acc: 0.2380\n",
      "Epoch 98/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0600 - acc: 0.3151 - val_loss: 0.0689 - val_acc: 0.2696\n",
      "Epoch 99/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0581 - acc: 0.3152 - val_loss: 0.0706 - val_acc: 0.2399\n",
      "Epoch 100/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0576 - acc: 0.3104 - val_loss: 0.0683 - val_acc: 0.2325\n",
      "Epoch 101/10000\n",
      "63/63 [==============================] - 14s 230ms/step - loss: 0.0562 - acc: 0.3212 - val_loss: 0.0693 - val_acc: 0.2743\n",
      "Epoch 102/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0563 - acc: 0.3193 - val_loss: 0.0733 - val_acc: 0.2238\n",
      "Epoch 103/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0555 - acc: 0.3280 - val_loss: 0.0695 - val_acc: 0.2361\n",
      "Epoch 104/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0549 - acc: 0.3183 - val_loss: 0.0698 - val_acc: 0.2587\n",
      "Epoch 105/10000\n",
      "63/63 [==============================] - 15s 242ms/step - loss: 0.0539 - acc: 0.3179 - val_loss: 0.0653 - val_acc: 0.2790\n",
      "Epoch 106/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0538 - acc: 0.3308 - val_loss: 0.0734 - val_acc: 0.2420\n",
      "Epoch 107/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0532 - acc: 0.3285 - val_loss: 0.0748 - val_acc: 0.2243\n",
      "Epoch 108/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0552 - acc: 0.3243 - val_loss: 0.0795 - val_acc: 0.2371\n",
      "Epoch 109/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0532 - acc: 0.3433 - val_loss: 0.0807 - val_acc: 0.2473\n",
      "Epoch 110/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0523 - acc: 0.3342 - val_loss: 0.0740 - val_acc: 0.3558\n",
      "Epoch 111/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0521 - acc: 0.3370 - val_loss: 0.0772 - val_acc: 0.2058\n",
      "Epoch 112/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0516 - acc: 0.3313 - val_loss: 0.0715 - val_acc: 0.2218\n",
      "Epoch 113/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0514 - acc: 0.3335 - val_loss: 0.0733 - val_acc: 0.2321\n",
      "Epoch 114/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0509 - acc: 0.3326 - val_loss: 0.0768 - val_acc: 0.1987\n",
      "Epoch 115/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0505 - acc: 0.3345 - val_loss: 0.0693 - val_acc: 0.2868\n",
      "Epoch 116/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0502 - acc: 0.3243 - val_loss: 0.0696 - val_acc: 0.2638\n",
      "Epoch 117/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0499 - acc: 0.3246 - val_loss: 0.0747 - val_acc: 0.2011\n",
      "Epoch 118/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0496 - acc: 0.3294 - val_loss: 0.0734 - val_acc: 0.2081\n",
      "Epoch 119/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.1827 - acc: 0.2463 - val_loss: 477.2974 - val_acc: 0.0412\n",
      "Epoch 120/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 14s 229ms/step - loss: 0.1261 - acc: 0.2623 - val_loss: 8.4812 - val_acc: 0.0527\n",
      "Epoch 121/10000\n",
      "63/63 [==============================] - 14s 230ms/step - loss: 0.1164 - acc: 0.2611 - val_loss: 0.2312 - val_acc: 0.2791\n",
      "Epoch 122/10000\n",
      "63/63 [==============================] - 14s 230ms/step - loss: 0.1099 - acc: 0.2681 - val_loss: 0.1394 - val_acc: 0.3620\n",
      "Epoch 123/10000\n",
      "63/63 [==============================] - 14s 230ms/step - loss: 0.1052 - acc: 0.2891 - val_loss: 0.1183 - val_acc: 0.3608\n",
      "Epoch 124/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.1011 - acc: 0.2903 - val_loss: 0.1136 - val_acc: 0.3119\n",
      "Epoch 125/10000\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.0977 - acc: 0.2960 - val_loss: 0.1133 - val_acc: 0.4114\n",
      "Epoch 126/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0961 - acc: 0.2925 - val_loss: 0.1029 - val_acc: 0.3886\n",
      "Epoch 127/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0953 - acc: 0.2971 - val_loss: 0.0993 - val_acc: 0.3782\n",
      "Epoch 128/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0949 - acc: 0.2966 - val_loss: 0.0969 - val_acc: 0.3311\n",
      "Epoch 129/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0945 - acc: 0.2956 - val_loss: 0.0962 - val_acc: 0.3201\n",
      "Epoch 130/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0942 - acc: 0.2954 - val_loss: 0.0953 - val_acc: 0.3332\n",
      "Epoch 131/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0938 - acc: 0.2971 - val_loss: 0.0937 - val_acc: 0.3081\n",
      "Epoch 132/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0934 - acc: 0.2981 - val_loss: 0.0949 - val_acc: 0.2759\n",
      "Epoch 133/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0930 - acc: 0.2984 - val_loss: 0.0939 - val_acc: 0.2733\n",
      "Epoch 134/10000\n",
      "63/63 [==============================] - 14s 230ms/step - loss: 0.0927 - acc: 0.2974 - val_loss: 0.0932 - val_acc: 0.2932\n",
      "Epoch 135/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0925 - acc: 0.2961 - val_loss: 0.0929 - val_acc: 0.3243\n",
      "Epoch 136/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0919 - acc: 0.3008 - val_loss: 0.0919 - val_acc: 0.3227\n",
      "Epoch 137/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0915 - acc: 0.3003 - val_loss: 0.0923 - val_acc: 0.3176\n",
      "Epoch 138/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0911 - acc: 0.2998 - val_loss: 0.0921 - val_acc: 0.3182\n",
      "Epoch 139/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0908 - acc: 0.3009 - val_loss: 0.0911 - val_acc: 0.3214\n",
      "Epoch 140/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0903 - acc: 0.3009 - val_loss: 0.0910 - val_acc: 0.3207\n",
      "Epoch 141/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0898 - acc: 0.3017 - val_loss: 0.0914 - val_acc: 0.3168\n",
      "Epoch 142/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0896 - acc: 0.3019 - val_loss: 0.0904 - val_acc: 0.3354\n",
      "Epoch 143/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0891 - acc: 0.3023 - val_loss: 0.0895 - val_acc: 0.3297\n",
      "Epoch 144/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0887 - acc: 0.3027 - val_loss: 0.0904 - val_acc: 0.3016\n",
      "Epoch 145/10000\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.0883 - acc: 0.3035 - val_loss: 0.0884 - val_acc: 0.3887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f84eaf3d978>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_stack = 1\n",
    "epochs = 10000\n",
    "sigma = 1.2\n",
    "\n",
    "model.fit_generator(train_gen.generator(batch_size, n_stack, sigma=sigma), len(train_gen)//batch_size,\n",
    "                    validation_data=valid_gen.generator(batch_size, n_stack, sigma=sigma), \n",
    "                    validation_steps=len(valid_gen)//batch_size,\n",
    "                    epochs=epochs, \n",
    "                    callbacks=my_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0028034717697664746, 0.3845100402832031]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(test_gen.generator(1,n_stack,sigma=sigma), steps=len(test_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
